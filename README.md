## MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in Code LLMs for Automated Program Repair
This repository contains the replication package for the paper  
[**MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in Code LLMs for Automated Program Repair** (ICSME 2024)](https://arxiv.org/abs/2408.09568),  
including the code, datasets, and instructions for running the experiments.

## Installing the Required Packages
To set up the environment, create a conda environment and install the required packages using the `environment.yml` file with the following commands:

```bash
conda env create -f environment.yml
conda activate shared
```

## Model and Dataset Setup
Download the models from [**starcoder2-3b**](https://huggingface.co/bigcode/starcoder2-3b) and [**granite-3b-code-base**](https://huggingface.co/ibm-granite/granite-3b-code-base-2k) and place downloaded files in the models directory. You can use other generative models considering the available amount of resources you can access to.

Both the original HumanEvalFix dataset and its perturbed version are available in their respective folders within the data directory. If you wish to regenerate the perturbed HumanEvalFix dataset, follow the instructions provided in the evaluation directory of the ReCode repository. When using the ReCode repository to generate datasets, set the --datasets argument to `humanevalfix` to ensure the correct base dataset is used. All other options and settings should follow the original instructions provided in the ReCode repository.

## LoRA Tuning Task-Specific Adapters
To fine-tune the models using LoRA adapters, use the `lora-tuning.py` script located in the `mergerrepair/tune/` directory.  
Ensure you select the correct config file corresponding to the model you're tuning. Before running the script, update the paths inside the script to the model, data, and other required files appropriately.

We use the same hyperparameter values as those reported in the original studies for each model. For `starcoder2-3b`, we follow the defaults from [**starcoder2**](https://github.com/bigcode-project/starcoder2), and for the `granite-3b-code-base` model, we use the values from [**granite-code models**](https://arxiv.org/pdf/2405.04324v1).

After fine-tuning each model for a task, a separate LoRA adapter will be saved in the `out/` directory. These adapters can then be used in the merging script with different merging methods in the next step.

## Merging Adapters
To merge the task-specific adapters, use the `merger.py` script located in the `merge/` directory.  
This script allows you to merge different task-specific adapters for each of the three RQs.  
Ensure that you provide the correct path to each adapter folder. Once set, simply run the script using the method corresponding to the specific RQ.

The merged adapters generated by this script will be used later in the evaluation phase by the evaluation scripts.

## Evaluating the Models using Adapters

### Pass@k
To evaluate the adapters on the HumanEvalFix dataset and obtain the pass@k score, use the `bigcode-evaluation-harness` library.  
Follow the instructions in the README of that repository located in the `evaluation/` directory to install and configure the required dependencies.

To run the evaluation, use the `run_evaluation.sh` script. This script evaluates the model on a single task-specific or merged adapter.

To evaluate multiple adapters sequentially, use the `run_evaluation_sequential.sh` script. In this script, you can specify a list of tasks along with the merging method, RQ, and model. The evaluation will then run for all selected tasks in one go.

| Option         | Value                                                              |
|----------------|--------------------------------------------------------------------|
| RQ             | `rq1`, `rq2`, `rq3`                                                |
| model          | `starcoder2-3b`, `granite-3b-code-base`                            |
| merging method | `weight-averaging`, `ties`, `dare_ties`                            |

### RobustPass@k

To evaluate the robustness of the models, use the perturbed version of the HumanEvalFix dataset.  
This dataset is already provided in the `data/` directory, but if you wish to regenerate it, you can use the ReCode source code located in the `evaluations/` directory.  
Follow the instructions in the README of that codebase to install the required libraries and configure the prerequisites.

Note: While ReCode is used to generate the perturbed dataset, it is **not compatible** with PEFT methods like LoRA. Therefore, for evaluation, we use the `bigcode-evaluation-harness` library, which supports adapters and PEFT methods.

To obtain RobustPass@k scores, run the `run_evaluation_recode.sh` script.

The perturbed datasets include four categories of perturbations: `format`, `func_name`, `natgen`, and `nlaugmenter`.  
For each adapter, you need to run four separate evaluations, one for each category.  
Use the following format for the `tasks` argument in the script: `perturbed-humaneval-{category}-num_seeds_5`. Replace `{category}` with one of the four perturbation types mentioned above.

All evaluation outputs will be saved in the `out/` directory under the `bigcode-evaluation-harness/` folder.  
The current contents reflect the results from our experiments. Re-running the evaluation scripts will overwrite them with new results.

Due to the large number of outputs, weâ€™ve included several scripts in this directory to gather, analyze, and format the results used in the paper.  
These are optional and only needed if you want to reproduce the exact result formatting as shown in the paper.

Please cite our paper if you find it helpful in your work:
```
@article{dehghan2024mergerepair,
  title={Mergerepair: An exploratory study on merging task-specific adapters in code llms for automated program repair},
  author={Dehghan, Meghdad and Wu, Jie JW and Fard, Fatemeh H and Ouni, Ali},
  journal={arXiv preprint arXiv:2408.09568},
  year={2024}
}

```